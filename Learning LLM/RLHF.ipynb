{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1. \\\n",
    "    torchdata==0.5.1 --quiet\n",
    "%pip install  \\\n",
    "    transformers==4.27.2 \\\n",
    "    datasets==2.11.0 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    # loralib==0.1.1 \\\n",
    "    peft==0.3.0 /\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install trl==0.4.4 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline,AutoTokenizer,AutoModelForSequenceClassification,AutoModelForSeq2SeqLM,GenerationConfig\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel,PeftConfig,LoraConfig,TaskType\n",
    "\n",
    "from trl import PPOTrainer,PPOConfig,AutoModelForSeq2SeqLMWithValueHead\n",
    "from trl import create_reference_model\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=0 if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_dataset_name=\"knkarthick/dialogsum\"\n",
    "dataset_original=load_dataset(huggingface_dataset_name)\n",
    "dataset_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model_name='google/flan-t5-base'\n",
    "model=AutoModelForSeq2SeqLM.from_pretrained(model_name,torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(model_name,device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limiting(dataset,min_length,max_length):\n",
    "    filtered_examples = dataset.filter(lambda example: min_length <= len(example[\"dialogue\"]) <= max_length)\n",
    "    return filtered_examples\n",
    "def build_dataset(model_name,dataset_name,min_length,max_length):\n",
    "    dataset=load_dataset(dataset_name,split=\"train\")\n",
    "    (limiting(dataset,min_length,max_length))\n",
    "    tokenizer=AutoTokenizer.from_pretrained(model_name,device_map=\"auto\")\n",
    "    def tokenize(sample):\n",
    "        prompt=f\"\"\"\n",
    "Summarize the following:\n",
    "{sample[\"dialogue\"]}\n",
    "Summary:\n",
    "\"\"\"\n",
    "        sample[\"input_ids\"]=tokenizer.encode(prompt)\n",
    "        sample[\"query\"]=tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "    dataset=dataset.map(tokenize,batched=False)\n",
    "    dataset.set_format(type=\"torch\")\n",
    "\n",
    "    dataset_splits=dataset.train_test_split(test_size=0.2,seed=42,shuffle=False)\n",
    "    return dataset_splits\n",
    "dataset=build_dataset(model_name,huggingface_dataset_name,200,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config=LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q\",\"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"None\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM #FLAN-T5\n",
    ")\n",
    "peft_model=PeftModel.from_pretrained(model=model,model_id=model_name,LoraConfig=lora_config,torch_dtype=torch.bfloat16,device_map=\"auto\",is_trainable=True)\n",
    "# peft_model=get_peft_model(model,lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_model=AutoModelForSeq2SeqLMWithValueHead.from_pretrained(peft_model,torch_dtype=torch.bfloat16,is_trainable=True)\n",
    "ref_model=create_reference_model(ppo_model)\n",
    "#jo model RL initialized weights wala hota hai usse aise load krte hai withvaluehead wale se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_model_name=\"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "toxic_tokenizer=AutoTokenizer.from_pretrained(toxic_model_name,device_map=\"auto\")\n",
    "toxic_model=AutoModelForSequenceClassification.from_pretrained(toxic_model_name,device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text=\"so beautiful so elegant just looking like a wow\"\n",
    "prepared_test_text=toxic_tokenizer(test_text,return_tensors=\"pt\").input_ids\n",
    "logits=toxic_model(input_ids=prepared_test_text).logits\n",
    "probabilites=logits.softmax(dim=-1).to_list()\n",
    "#logits are returned in order not hate and hate logits,we will use logits of nothate as reward\n",
    "nothate_reward=logits[:2].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate_method\n",
    "# instead of tokenizing and input id we can use pipeline which does this\n",
    "sentiment_pipeline=pipeline(\"sentiment-analysis\",model=toxic_model,auto_map=\"device\")\n",
    "sentiment_pipeline(test_text)#returns logits\n",
    "\n",
    "# if you want to return logits with somedifferenct method, then use kwargs\n",
    "logits_kwargs={\n",
    "    \"top_k\":None,#returns all logits\n",
    "    \"function_to_apply\":None,\n",
    "    \"batch_size\":16\n",
    "}\n",
    "probab_kwargs={\n",
    "    \"top_k\":None,\n",
    "    \"function_to_apply\":\"softmax\",\n",
    "    \"batch_size\":16   #doubt\n",
    "}\n",
    "sentiment_pipeline(test_text,**logits_kwargs)#returns logits as earlier case\n",
    "sentiment_pipeline(test_text,**probab_kwargs)#returns probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_calculator=evaluate.load(\n",
    "    \"toxicity\",toxic_model,\n",
    "    module_type=\"measurement\",\n",
    "    toxic_label=\"hate\"\n",
    "    ) #DOUBT documentation me toh toxic_label jaisa koi parameter nhi hai        \n",
    "toxicity_score=toxicity_calculator.compute(predictions=[test_text])\n",
    "print(toxicity_score[\"toxicity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config=PPOConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=1.41e-5,\n",
    "    max_ppo_epochs=1,\n",
    "    batch_size=16,\n",
    "    mini_batch_size=4\n",
    ")\n",
    "def collator(data):\n",
    "    return dict((key,[d[key] for d in data])for key in data[0])\n",
    "ppo_trainer=PPOTrainer(\n",
    "    config=config,\n",
    "    ref_model=ref_model,\n",
    "    model=ppo_model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset[\"train\"],\n",
    "    data_collator=collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_min_length=100\n",
    "output_max_length=400\n",
    "output_length_sampler=LengthSampler(output_min_length,output_max_length)\n",
    "generation_kwargs={\n",
    "    \"min_length\":5,\n",
    "    \"top_k\":0.0,\n",
    "    \"top_p\":1.0,\n",
    "    \"do_sample\":True\n",
    "}\n",
    "reward_kwargs={\n",
    "    \"top_k\":None,\n",
    "    \"functions_to_apply\":\"none\",\n",
    "    \"batch_size\":16\n",
    "}\n",
    "max_ppo_steps=10\n",
    "\n",
    "for step,batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    if step>=max_ppo_steps:\n",
    "        break\n",
    "    prompt_tensors=batch[\"input_ids\"]\n",
    "    summary_tensors=[]\n",
    "    for prompt_tensors in prompt_tensors:\n",
    "        max_new_tokens=output_length_sampler()\n",
    "        generation_kwargs[\"max_new_tokens\"]=max_new_tokens\n",
    "        summary=ppo_trainer.generate(prompt_tensors,**generation_kwargs)\n",
    "        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n",
    "\n",
    "    batch[\"response\"]=[tokenizer.decode(r.squeeze())for r in summary_tensors]\n",
    "\n",
    "    query_response_pairs=[q+r for q,r in zip(batch[\"query\"],batch[\"response\"])]\n",
    "    rewards=sentiment_pipeline(query_response_pairs,**reward_kwargs)\n",
    "    reward_tensors=[torch.tensor(reward[0][\"score\"]) for reward in rewards]#0 is used for not_hate_index\n",
    "    stats=ppo_trainer.step(prompt_tensors,summary_tensors,reward_tensors)\n",
    "    ppo_trainer.log_stats(stats,batch,reward_tensors)\n",
    "    print(f'objective/kl:{stats[\"objective/kl\"]}')\n",
    "    print(f'ppo/returns/mean:{stats[\"ppo/returns/mean\"]}')\n",
    "    print(f'ppo/policy/advantages_mean:{stats[\"ppo/policy/advantages_mean\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
