{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/outbreakkp/.local/lib/python3.10/site-packages (23.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1. \\\n",
    "    torchdata==0.5.1 --quiet\n",
    "%pip install  \\\n",
    "    transformers==4.27.2 \\\n",
    "    datasets==2.11.0 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    loralib==0.1.1 \\\n",
    "    peft==0.3.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM,AutoTokenizer,GenerationConfig,TrainingArguments,Trainer\n",
    "import time\n",
    "import torch\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/outbreakkp/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "100%|██████████| 3/3 [00:00<00:00, 504.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_dataset_name=\"knkarthick/dialogsum\"\n",
    "dataset=load_dataset(huggingface_dataset_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "original_model=AutoModelForSeq2SeqLM.from_pretrained(model_name,torch_dtype=torch.bfloat16)\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247577856\n",
      "247577856\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in original_model.parameters() )\n",
    "print(pytorch_total_params)\n",
    "pytorch_total_params_traiable = sum(p.numel() for p in original_model.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params_traiable)\n",
    "#tensors hote hai unke andr parameters hote hai , model.parameters() itereator return krta hai tensors pe iterate krne ka unpe for p in model.pramaeters krke iterate kia fir tensors ke andr jitne elements hai unhe parameters kehte hai unka count lene ke lie tensor.numel kia aur sb tensors ka sum krne ke lie sum lgaya bhaar, is grad to check if that param is trainable category wala h ya nhi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(example):\n",
    "    start=\"Summarize the following Conversation \\n\\n\"\n",
    "    end=\"\\n\\nSummary:\"\n",
    "    prompt=[start+dialogue+end for dialogue in example[\"dialogue\"]]\n",
    "    example['input_ids']=tokenizer(prompt,padding=\"max_length\",truncation=True,return_tensors=\"pt\").input_ids\n",
    "    example['labels']=tokenizer(example['summary'],padding=\"max_length\",truncation=True,return_tensors=\"pt\").input_ids\n",
    "    return example  \n",
    "\n",
    "tokenized_dataset=dataset.map(tokenize_dataset,batched=True) #map() maps or gives access to each example of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking subset of tokenized_data to save time\n",
    "\n",
    "tokenized_dataset['train'] = tokenized_dataset['train'].shuffle(seed=42).select(range(1))\n",
    "tokenized_dataset['test'] = tokenized_dataset['test'].shuffle(seed=42).select(range(2))\n",
    "tokenized_dataset['validation'] = tokenized_dataset['validation'].shuffle(seed=42).select(range(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = tokenized_dataset['train'].column_names\n",
    "print(column_names)\n",
    "tokenized_dataset=tokenized_dataset.remove_columns(['id', 'dialogue', 'summary', 'topic', ])\n",
    "column_names = tokenized_dataset['train'].column_names\n",
    "print(column_names)\n",
    "tokenized_dataset['validation'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir='output-dir-folder'#where the model will be stored generally of size of GBs\n",
    "trainingargs=TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    max_steps=1,\n",
    "    logging_steps=1\n",
    "    )\n",
    "trainer=Trainer(\n",
    "    model=original_model,\n",
    "    args=trainingargs,\n",
    "    train_dataset=tokenize_dataset[\"train\"],\n",
    "    eval_dataset=tokenize_dataset[\"validation\"]\n",
    ")#creating instance of trainer which is called in next cell,this is given by transformers library of hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge=evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_fine_tuned_model_results=rouge.compute(\n",
    "    predictions=clear_output,#instead of clear_output use model ouputs\n",
    "    references=dataset['summary'],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True#explains running , run ,ran are same \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig,get_peft_model,TaskType\n",
    "lora_config=LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\",\"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"None\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM #FLAN-T5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model=get_peft_model(original_model,lora_config)\n",
    "pytorch_total_params2 = sum(p.numel() for p in peft_model.parameters() )\n",
    "print(pytorch_total_params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir='output-dir-folder'#where the model will be stored generally of size of GBs\n",
    "trainingargsofpeft=TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    max_steps=1,\n",
    "    logging_steps=1\n",
    "    )\n",
    "peft_trainer=Trainer(\n",
    "    model=peft_model,\n",
    "    args=trainingargsofpeft,\n",
    "    train_dataset=tokenize_dataset[\"train\"],\n",
    "    eval_dataset=tokenize_dataset[\"validation\"]\n",
    ")#creating instance of trainer which is called in next cell,this is given by transformers library of hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_trainer.train()\n",
    "peft_trainer.model.save_pretrained('output-dir-folder')\n",
    "tokenizer.save_pretrained('output-dir-folder')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
